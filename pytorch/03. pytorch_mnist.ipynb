{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_pytorch_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7yAyT06_q4F"
      },
      "source": [
        "MLP로 MNIST 분류  \n",
        "https://wikidocs.net/61073"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhaqASu3uilF"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyE_IY8E-wlc"
      },
      "source": [
        "mnist = fetch_openml('mnist_784', version=1, cache=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uJ7qq1x-8qw"
      },
      "source": [
        "mnist.target = mnist.target.astype(np.int8)  # .target : label 값"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kuOsWPM_CXS"
      },
      "source": [
        "X = mnist.data / 255  # 0-255값을 [0,1] 구간으로 정규화\n",
        "y = mnist.target"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "k9f0Bicm_G0r",
        "outputId": "b8ccd090-9ca9-48d0-ac7a-1c8580731338"
      },
      "source": [
        "plt.imshow(X[0].reshape(28, 28), cmap='gray')\n",
        "print(\"이 이미지 데이터의 레이블은 {:.0f}이다\".format(y[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 이미지 데이터의 레이블은 5이다\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwrL6-s9_HqT"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh2ppGyu_JuV"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=0)\n",
        "\n",
        "X_train = torch.Tensor(X_train)\n",
        "X_test = torch.Tensor(X_test)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "ds_train = TensorDataset(X_train, y_train)\n",
        "ds_test = TensorDataset(X_test, y_test)\n",
        "\n",
        "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
        "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0XIalyQ_L9r",
        "outputId": "fcf33e24-230f-417e-f750-339e77064c3f"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential()\n",
        "model.add_module('fc1', nn.Linear(28*28*1, 100))\n",
        "model.add_module('relu1', nn.ReLU())\n",
        "model.add_module('fc2', nn.Linear(100, 100))\n",
        "model.add_module('relu2', nn.ReLU())\n",
        "model.add_module('fc3', nn.Linear(100, 10))\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY1Nql87_ODw"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# 오차함수 선택\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 가중치를 학습하기 위한 최적화 기법 선택\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT7OLwoI_Tr3"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()  # 신경망을 학습 모드로 전환\n",
        "\n",
        "    # 데이터로더에서 미니배치를 하나씩 꺼내 학습을 수행\n",
        "    for data, targets in loader_train:\n",
        "\n",
        "        optimizer.zero_grad()  # 경사를 0으로 초기화\n",
        "        outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
        "        loss = loss_fn(outputs, targets)  # 출력과 훈련 데이터 정답 간의 오차를 계산\n",
        "        loss.backward()  # 오차를 역전파 계산\n",
        "        optimizer.step()  # 역전파 계산한 값으로 가중치를 수정\n",
        "\n",
        "    print(\"epoch{}：완료\\n\".format(epoch))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5dnFV9c_WAQ"
      },
      "source": [
        "def test():\n",
        "    model.eval()  # 신경망을 추론 모드로 전환\n",
        "    correct = 0\n",
        "\n",
        "    # 데이터로더에서 미니배치를 하나씩 꺼내 추론을 수행\n",
        "    with torch.no_grad():  # 추론 과정에는 미분이 필요없음\n",
        "        for data, targets in loader_test:\n",
        "\n",
        "            outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
        "\n",
        "            # 추론 계산\n",
        "            _, predicted = torch.max(outputs.data, 1)  # 확률이 가장 높은 레이블이 무엇인지 계산\n",
        "            correct += predicted.eq(targets.data.view_as(predicted)).sum()  # 정답과 일치한 경우 정답 카운트를 증가\n",
        "\n",
        "    # 정확도 출력\n",
        "    data_num = len(loader_test.dataset)  # 데이터 총 건수\n",
        "    print('\\n테스트 데이터에서 예측 정확도: {}/{} ({:.0f}%)\\n'.format(correct,\n",
        "                                                   data_num, 100. * correct / data_num))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "421HgnFc_Yiz",
        "outputId": "5dfda0fb-6d7f-4aea-fbc7-2b7971e14129"
      },
      "source": [
        "test()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "테스트 데이터에서 예측 정확도: 877/10000 (9%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZtm_-dv_bG7",
        "outputId": "9c1a2975-2df4-4597-99e0-299dfa67b1ab"
      },
      "source": [
        "for epoch in range(3):\n",
        "    train(epoch)\n",
        "\n",
        "test()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0：완료\n",
            "\n",
            "epoch1：완료\n",
            "\n",
            "epoch2：완료\n",
            "\n",
            "\n",
            "테스트 데이터에서 예측 정확도: 9570/10000 (96%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "sgBvy1q5_d39",
        "outputId": "3a3f8e2b-656c-4838-8b48-19560cfad2f3"
      },
      "source": [
        "index = 2018\n",
        "\n",
        "model.eval()  # 신경망을 추론 모드로 전환\n",
        "data = X_test[index]\n",
        "output = model(data)  # 데이터를 입력하고 출력을 계산\n",
        "_, predicted = torch.max(output.data, 0)  # 확률이 가장 높은 레이블이 무엇인지 계산\n",
        "\n",
        "print(\"예측 결과 : {}\".format(predicted))\n",
        "\n",
        "X_test_show = (X_test[index]).numpy()\n",
        "plt.imshow(X_test_show.reshape(28, 28), cmap='gray')\n",
        "print(\"이 이미지 데이터의 정답 레이블은 {:.0f}입니다\".format(y_test[index]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 결과 : 2\n",
            "이 이미지 데이터의 정답 레이블은 2입니다\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz0jQwsYMofn"
      },
      "source": [
        "CNN으로 MNIST 분류  \n",
        "https://wikidocs.net/63565"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzEKHLKQ_giO"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqlHJ8-AMyNp"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 랜덤 시드 고정\n",
        "torch.manual_seed(777)\n",
        "\n",
        "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fl9BNBbM0zj"
      },
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgUwl9GPM4j2",
        "outputId": "bd403134-41f6-4a59-cd3a-8be3d37a1b9e"
      },
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
        "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n",
        "                          transform=transforms.ToTensor(), # 텐서로 변환\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
        "                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n",
        "                         transform=transforms.ToTensor(), # 텐서로 변환\n",
        "                         download=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fhWvnMfM75d"
      },
      "source": [
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgrUxXoNM8b"
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # 첫번째층\n",
        "        # ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 두번째층\n",
        "        # ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
        "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
        "\n",
        "        # 전결합층 한정으로 가중치 초기화\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj9bqVlFNPz4",
        "outputId": "d0c79ea5-e07b-4ce5-b114-b3f060649859"
      },
      "source": [
        "model = CNN().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_batch = len(data_loader)\n",
        "print('총 배치의 수 : {}'.format(total_batch))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 배치의 수 : 600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw74u-QFNYce",
        "outputId": "f2333d53-c57a-4fd8-fd0e-c53419f0c134"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X: 미니 배치, Y: 레이블.\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] cost = 0.225638643\n",
            "[Epoch:    2] cost = 0.0630241856\n",
            "[Epoch:    3] cost = 0.0462851562\n",
            "[Epoch:    4] cost = 0.0374466665\n",
            "[Epoch:    5] cost = 0.0314616188\n",
            "[Epoch:    6] cost = 0.0261270478\n",
            "[Epoch:    7] cost = 0.0219386742\n",
            "[Epoch:    8] cost = 0.0184466876\n",
            "[Epoch:    9] cost = 0.0165075548\n",
            "[Epoch:   10] cost = 0.0135030029\n",
            "[Epoch:   11] cost = 0.0103755686\n",
            "[Epoch:   12] cost = 0.00947520789\n",
            "[Epoch:   13] cost = 0.00858631078\n",
            "[Epoch:   14] cost = 0.00681982981\n",
            "[Epoch:   15] cost = 0.00774352159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3S-yOGmNfGC",
        "outputId": "3eeb8d9c-ab2c-4bcf-d6f4-85e2be5d2cc4"
      },
      "source": [
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9853000044822693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_KLclTMPE5A"
      },
      "source": [
        "Deep CNN으로 MNIST 분류  \n",
        "https://wikidocs.net/63618"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HC0Jyq7TFfe"
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.keep_prob = 0.5\n",
        "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L3 ImgIn shape=(?, 7, 7, 64)\n",
        "        #    Conv      ->(?, 7, 7, 128)\n",
        "        #    Pool      ->(?, 4, 4, 128)\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
        "\n",
        "        # L4 FC 4x4x128 inputs -> 625 outputs\n",
        "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            self.fc1,\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=1 - self.keep_prob))\n",
        "        # L5 Final FC 625 inputs -> 10 outputs\n",
        "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
        "        out = self.layer4(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RobLC-lKeIQA"
      },
      "source": [
        "model = CNN().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_batch = len(data_loader)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naP9CkmJTYFb",
        "outputId": "f18ba00f-92b0-49aa-9608-bc9fae327000"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X: 미니 배치, Y: 레이블.\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] cost = 0.188000172\n",
            "[Epoch:    2] cost = 0.048595272\n",
            "[Epoch:    3] cost = 0.0356774479\n",
            "[Epoch:    4] cost = 0.0269749034\n",
            "[Epoch:    5] cost = 0.0208106544\n",
            "[Epoch:    6] cost = 0.0190930441\n",
            "[Epoch:    7] cost = 0.0171050243\n",
            "[Epoch:    8] cost = 0.0135025494\n",
            "[Epoch:    9] cost = 0.0132994847\n",
            "[Epoch:   10] cost = 0.0114710173\n",
            "[Epoch:   11] cost = 0.0107851494\n",
            "[Epoch:   12] cost = 0.00718139298\n",
            "[Epoch:   13] cost = 0.0083659282\n",
            "[Epoch:   14] cost = 0.00801821984\n",
            "[Epoch:   15] cost = 0.00846904423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqfXHX0vTk9R",
        "outputId": "e9bbf6f6-e66d-4fd1-a4dd-936099a12a6e"
      },
      "source": [
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9851999878883362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU3nIjMdV6sq"
      },
      "source": [
        "# Batch Normalization 추가\n",
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.keep_prob = 0.5\n",
        "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L3 ImgIn shape=(?, 7, 7, 64)\n",
        "        #    Conv      ->(?, 7, 7, 128)\n",
        "        #    Pool      ->(?, 4, 4, 128)\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
        "\n",
        "        # L4 FC 4x4x128 inputs -> 625 outputs\n",
        "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            self.fc1,\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=1 - self.keep_prob))\n",
        "        # L5 Final FC 625 inputs -> 10 outputs\n",
        "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
        "        out = self.layer4(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLSAZfoeV9R"
      },
      "source": [
        "model = CNN().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_batch = len(data_loader)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANNQArpsWNb-",
        "outputId": "a041b9be-5160-44ba-8de6-48f1251de28e"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X: 미니 배치, Y: 레이블.\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] cost = 0.197541639\n",
            "[Epoch:    2] cost = 0.0650532842\n",
            "[Epoch:    3] cost = 0.0517384298\n",
            "[Epoch:    4] cost = 0.0409297198\n",
            "[Epoch:    5] cost = 0.0359429382\n",
            "[Epoch:    6] cost = 0.0325355828\n",
            "[Epoch:    7] cost = 0.027208766\n",
            "[Epoch:    8] cost = 0.0263407715\n",
            "[Epoch:    9] cost = 0.0224186387\n",
            "[Epoch:   10] cost = 0.0201063566\n",
            "[Epoch:   11] cost = 0.0166624319\n",
            "[Epoch:   12] cost = 0.014817778\n",
            "[Epoch:   13] cost = 0.0155637376\n",
            "[Epoch:   14] cost = 0.011714289\n",
            "[Epoch:   15] cost = 0.0122901592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEuumiI1WPrm",
        "outputId": "59b78fc7-3586-4d8c-95fe-5583af8b19c8"
      },
      "source": [
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9919999837875366\n"
          ]
        }
      ]
    }
  ]
}